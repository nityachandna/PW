{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a8b43b0-0095-4c33-bb4a-5dc6ff4dac54",
   "metadata": {},
   "source": [
    "## ML ASSIGNMENT (16th March)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e4c3a-8448-428f-b34b-740048d32644",
   "metadata": {},
   "source": [
    "#### Q1) Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa6a8a8-a749-4a45-8c22-369d83a48817",
   "metadata": {},
   "outputs": [],
   "source": [
    "Overfitting:\n",
    "When a model performs very well for training data but has poor performance with test data (new data), it is known as overfitting.\n",
    "Overfitting can happen due to low bias and high variance.\n",
    "An overfit model can give inaccurate predictions and cannot perform well for all types of new data.\n",
    "\n",
    "Underfitting:\n",
    "When a model has not learned the patterns in the training data well and is unable to generalize well on the new data, it is known as underfitting. \n",
    "Underfitting occurs due to high bias and low variance.\n",
    "An underfit model has poor performance on the training data and will result in unreliable predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559fa028-86d9-435c-856f-b922a77615c2",
   "metadata": {},
   "source": [
    "#### Q2) How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3c5996-e26b-48d6-92a4-a3f23791c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ways to Tackle Overfitting:\n",
    "1. Cross-validation [K-Fold] \n",
    "2. Regularization techniques such as Lasso and Ridge (reduces number of features/input parameters, limits variance) \n",
    "3. Training model with sufficient (more) data\n",
    "4. Ensembling techniques like Bagging and Boosting (several models are aggregated to produce one optimal predictive model)\n",
    "5. Early stopping - to pause the model's training before memorizing noise and random fluctuations from the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee367a-62a9-48b3-b580-a15ac88014b0",
   "metadata": {},
   "source": [
    "#### Q3) Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edffcbde-ad3d-43c9-a333-988fbc100c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "When a model has not learned the patterns in the training data well and is unable to generalize well on the new data, it is known as underfitting.\n",
    "An underfit model has poor performance on the training data and will result in unreliable predictions. \n",
    "Underfitting occurs due to high bias and low variance.\n",
    "\n",
    "Reasons for Underfitting:\n",
    "1. Data used for training is not cleaned and contains noise (garbage values) in it\n",
    "2. The model has a high bias\n",
    "3. The size of the training dataset used is not enough\n",
    "4. The model is too simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491189a-bee7-4647-b80d-b70064ca8988",
   "metadata": {},
   "source": [
    "#### Q4) Explain the bias-variance tradeoff in machine learning.\n",
    "#### What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f82f758-8347-4123-b9ab-3560567bba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bias refers to the tendency of the model to make systematic errors. \n",
    "High bias can cause the model to underfit the data, leading to poor performance on both training and unseen data.\n",
    "\n",
    "Variance refers to the tendency of the model to make random errors. \n",
    "High variance can lead to overfitting, where the model captures noise in the training data and performs poorly on new, unseen data.\n",
    "\n",
    "Bias-variance tradeoff refers to the delicate balance between two sources of error in a predictive model: bias and variance.\n",
    "Bias and variance are inversely connected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86092e77-4c00-4545-ba89-2cc889152daf",
   "metadata": {},
   "source": [
    "#### Q5) Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "#### How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ef43c-67e8-4b0c-b999-b96ddd02a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Detecting underfitting:\n",
    "1) Training and test loss:\n",
    "If the model is underfitting, the loss for both training and validation will be considerably high.\n",
    "In other words, for an underfitting dataset, the training and the validation error will be high.\n",
    "2) Oversimplistic prediction graph: \n",
    "If a graph with the data points and the fitted curve is plotted, and the classifier curve is oversimplistic, then, most probably, your model is underfitting.\n",
    "In those cases, a more complex model should be tried out. \n",
    "\n",
    "Detecting overfitting:\n",
    "1) Use a resampling technique to estimate model accuracy. The most popular resampling technique is k-fold cross-validation\n",
    "2) Hold back a validation set. Once a model is trained on the training set, you can evaluate it on the validation dataset.\n",
    "Then compare the accuracy of the model in the training dataset and the validation dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89de5c7-0952-4295-a335-8d17aa17f02e",
   "metadata": {},
   "source": [
    "#### Q6) Compare and contrast bias and variance in machine learning. \n",
    "#### What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdecd03b-1e2f-490e-b26a-cda033e778f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "While making predictions, a difference occurs between prediction values made by the model and actual values/expected values.\n",
    "This difference is known as bias errors or errors due to bias. \n",
    "Whereas, variance tells that how much a random variable is different from its expected value.\n",
    "High variance models:\n",
    "Decision Trees, Support Vector Machines and k-Nearest Neighbors\n",
    "High bias models:\n",
    "Linear Regression and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134244d0-2805-4916-bb4b-8366377e3696",
   "metadata": {},
   "source": [
    "#### Q7) What is regularization in machine learning, and how can it be used to prevent overfitting? \n",
    "#### Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa19db2-7ed4-4402-97fa-937d3bddaec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization is a technique used to reduce errors by fitting the function appropriately on the given training set and avoiding overfitting.\n",
    "In this technique, we reduce the magnitude of the features by keeping the same number of features.\n",
    "The commonly used regularization techniques are: \n",
    "1) Lasso Regularization – L1 Regularization\n",
    "2) Ridge Regularization – L2 Regularization\n",
    "3) Elastic Net Regularization – L1 and L2 Regularization\n",
    "\n",
    "Lasso Regression:\n",
    "A regression model which uses the L1 Regularization technique is called LASSO (Least Absolute Shrinkage and Selection Operator) regression. \n",
    "Lasso Regression adds the “absolute value of magnitude” of the coefficient as a penalty term to the loss function (L). \n",
    "\n",
    "Ridge Regression:\n",
    "A regression model that uses the L2 regularization technique is called Ridge regression. \n",
    "Ridge regression adds the “squared magnitude” of the coefficient as a penalty term to the loss function(L).\n",
    "\n",
    "Elastic Net Regression:\n",
    "This model is a combination of L1 as well as L2 regularization. \n",
    "That implies that we add the absolute norm of the weights as well as the squared measure of the weights.\n",
    "With the help of an extra hyperparameter that controls the ratio of the L1 and L2 regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
